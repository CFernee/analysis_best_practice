---
title: "Reproducible Research"
subtitle: "Data Analysis Best Practice"
author: "Natalie Thurlby"
highlighter: highlight.js
hitheme: tomorrow
job: Data scientist, Jean Golding Institute
logo: 
mode: selfcontained
framework: io2012
url:
  assets: ../assets
  lib: ../librariesNew
widgets: mathjax
output: ioslides_presentation
---

```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
library(knitr)
options(width = 100)
opts_chunk$set(eval=T, results = 'markup', include=T, message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
# knit_hooks$set(inline = function(x) {
#   if(is.numeric(x)) {
#     round(x, getOption('digits'))
#   } else {
#     paste(as.character(x), collapse = ', ')
#   }
# })
knit_hooks$set(plot = knitr:::hook_plot_html)
```


## What is reproducibility?

These are the terms which most people seem to be using:

__Reproducible:__ Your research is reproducible if other people get the same results running the same analysis on your data.

__Replicabile:__ Your research replicates if other people repeat the experiment and their results are consistent with yours. 

___________

Being reproducible is a prerequisite for being replicable; __at a minimum your work should be reproducible__. 

Today we're going to try and __reproduce__ the results of a paper.

Good practices in data analysis will help you with both reproducibility and replicability (plus other benefits)!

---

## Reproducibility crisis

__Less than 40%__ of replications of well-known Psychology studies had significant results:

```{r, echo=F, out.width='100%'}
include_graphics('fig/psychology-replication-paper.jpg')
```
"Open Science Collaboration. "Estimating the reproducibility of psychological science." Science 349.6251 (2015): aac4716.

Only __11%__ of replications of well-known cancer biology studies had significant results. 

Begley, C. Glenn, and Lee M. Ellis. "Drug development: Raise standards for preclinical cancer research." Nature 483.7391 (2012): 531.

--- &twocol

## Most researchers think there is a problem with reproducibility

*** =left

```{r, echo=F, out.width='100%'}
include_graphics('fig/is-there-a-crisis.jpg')
```

*** =right

Baker, Monya. "1,500 scientists lift the lid on reproducibility." Nature News 533.7604 (2016): 452


---&twocol

## Most researchers have failed to reproduce a result
*** =left

```{r, echo=F, out.width='100%'}
include_graphics('fig/failed-to-reproduce.jpg')
```

*** =right

Of the 1576 scientists surveyed, __over 70% of scientists surveyed have experienced failure to reproduce other's results__ and __over 50% have failed to reproduce their own results__.

---

## Why should we be reproducible?

* It's the right thing to do - it gives us all better science.
* Helps you to avoid disaster/embarrassment.
* Saves you time in the long run
    * Helps you to write papers/your thesis.
    * Helps you continue your research by remembering what you did, so you or someone else can build on your previous work.
* Makes you a better researcher (because you know people can check your work, you will too)
* Good for your reputation (makes you more trustworthy). 

---

## Types of reproducibility problems

* Lacking information: data or methods not provided in full.
* Data Storage problems
* Analysis environment
* P-hacking (and HARKing)

--- &twocol
## Provide your methods and data!

*** =left
In order for your work to be reproducible, at minimum, you must provide enough information for another researcher to be able to reproduce it.

This requires you to __make your data and methods available!__

And __make what you provide usable to others__ by making sure that you store your data (with documentation) publicly wherever possible and provide a full description of your methodology as well as a script that others can run to check your results.

*** =right
If you don't then, great, but if sharing your code sounds scary:
* You could run your scripts through a linter ([lintr](https://cran.r-project.org/web/packages/lintr/index.html) in R - supported in RStudio) first to have it tell you about any errors that can be automatially detected.
* Everyone can improve at this stuff, so I think everyone finds it a bit scary. Share your stuff anyway, please!

```{r, echo=F, out.width='100%'}
include_graphics('fig/imposter.jpg')
```

---&twocol

## Data Storage 

*** =left
__Avoid:__
- Accidentally editing data or results
- Forgetting version or origin of data
- Making it hard or impossible for your data to be accessed and understood by others/your future self.

__Avoid because:__
- Overwriting data will affect your results. 
- No one will be able to reproduce your results (including you), even if they are right.

*** =right
__Suboptimal file names:__
* "data.csv""
* "mice friendliness.txt"
* "deprivation_final_actually_for_realsies.txt"
* "bees_measurements.xls"

```{r, echo=F, out.width='80%'}
include_graphics('fig/duke-scandal.jpg')
```

---&twocol

## Data Storage 

*** =left

__Anxiety-inducing:__
```{r, echo=F, out.width='90%'}
include_graphics('fig/directory_bad.jpg')
```

*** =right
__Pretty good:__
```{r, echo=F, out.width='90%'}
include_graphics('fig/directory_good.jpg')
```

* Organise files sensibly. 
* Make data folders read only. 
* Name files well. 
* Include all information you need to re-collect the data in a readme file.
* Store data in text format where possible (easy to share and to version control)

---&twocol

## Analysis environment - Scripts

*** =left
__Avoid:__
- Clicking through a GUI to run analysis.
- Using proprietry software.
- Writing scripts that no one can understand.
- Forgetting what your scripts do.

__Avoid because:__
- Will take you ages to repeat analysis if you need to.
- The order of doing things could change the results (results might be wrong).
- Other people will not be able to reproduce your work.
---&twocol

*** =right

__Solve by:__
- Writing scripts
- Writing scripts in non-proprietry software (i.e. R and python beat STATA and SPSS)
- Writing scripts that are easy to understand: comment your code or embed it in "notebooks" (eg. [Jupyter](http://jupyter.org/) or [R notebooks](https://rmarkdown.rstudio.com/r_notebooks) and name your variables sensibly. 
- Documenting your scripts (README files, comments, documented functions)

---&twocol

## Analysis environment - Version Control

*** =left
__Avoid:__
- Saving over earlier work without being able to get back to it 
- Having files named "analysis_version32_final_actually_final.R"

__Avoid because:__
- Nothing is worse than knowing your program worked a minute ago.
- You won't be able to remember which is the actually_actually_real_final_file.txt

*** =right
```{r, echo=F, out.width='50%'}
include_graphics('fig/final_version.jpg')
```

__Solve by:__
- <font color='red'>Using version control (i.e. Git)</font>

---&twocol
## P-values

*** =left

__What does it mean if p < 0.05?__
  * Your result is true?
  * There's a 95% chance your hypothesis is true?
  * There's a 5% (1 in 20) chance that your hypothesis testing has resulted in a false positive assuming that you're using the right measure and all your assumptions are correct?
  * Your result is interesting? 
  * Your result will be easier to publish?
  
> If p = 0.05, it means there is a 1 in 20 chance that your result is false.
> To put this another way: if the effect you are measuring is completely random, then one in 20 times you will get a false positive (p<0.05).

*** =right
```{r, echo=F, out.width='100%'}
include_graphics('fig/dice.jpg')
```

---
## Experiments - Simulation 1

Imagine that you are a researcher, you have your own dataset and you are going to test one scientific hypothesis. The hypothesis you have chosen is __NOT TRUE__, but of course you don't know that yet! 

__Rules:__
* Rolling a dice = testing a hypothesis (which isn't true) 
* Rolling a 1 => p<0.05 (a false positive)
* Rolling anything else => p>0.05 (a true negative)

__Instructions:__
* Roll a 20-sided dice once each.

__Results 1:__
* How many of you rolled a 1? (Got a false positive)

> What do you think the probability is of each of you rolling a 1?

---&twocol

## Experiments - Simulation 2

Imagine now that I am the PI of a lab and you are all my PhD students. I have one dataset with many different variables in it, and you are all testing very slighly different hypotheses for me on that dataset. 

*** =left
__Instructions:__
* Roll a 20-sided dice once each. 

__Results:__
* How many of you rolled a 1? (Got a false positive)

> What do you think the probability is of at least one of you rolling a 1?

*** =right
```{r, echo=F, out.width='100%'}
include_graphics('fig/forking_paths.jpg')
```

--- &twocol

## HARKing

*** =left
__HARKing = Hypothesising After Results are Known__

Instead of trying the same hypothesis with 20 different methodologies, you can get the same effect (finding erroneous "significant" results) by testing 20 different hypotheses and decide what you hypothesised afterwards.

*** =right

```{r, echo=F, out.width='45%'}
include_graphics('fig/jellybean_xkcd.jpg')
```

---

## HARKing example

```{r, echo=F, out.width='100%'}
include_graphics('fig/HARKing_real.jpg')
```

__Bold__ = abstract presented

Text = full description of experiment

---&twocol

## P-hacking

P-hacking is a catch-all term for making p-values appear smaller than they are. 

*** =left
__Examples of p-hacking:__
- HARKing
- Collecting samples until your sample size gives you p < 0.05
- Choosing a different test to get p<0.05

__Solutions to p-hacking:__
* Pre-registration and [Registered reports](https://cos.io/rr/) (saying what you analysis you will do in advance: more on this later)
* Multi-hypothesis corrections (e.g. [Bonferroni correction](https://en.wikipedia.org/wiki/Bonferroni_correction))
* Simulating data.

*** =right
```{r, echo=F, out.width='40%'}
include_graphics('fig/p_values_xkcd.jpg')
```
```{r, echo=F, out.width='50%'}
include_graphics('fig/p-curve.jpg')
```


---&twocol

## Summary 

*** =left
__Reproducible research is important because:__.
* It saves your time when you come back to your work.
* Its good for your reputation (by avoiding disaster, improving your motivation to do good work, and through open science badges, etc).
* It saves us from bad science.

*** =right
__Main causes of poor reproducibility:__
* (1) Poor data storage
* (2) Poor analysis pipeline
* (3) Lack of sharing data/methods
* (4) Poor use of p-values

(1)-(3) are just a matter of becoming familiar with the tools we're going to be using today. If you have any more questions about (4), you can email us <ask-jgi@bristol.ac.uk>

---&twocol


## Take home message:

If you don't do anything else, these three things will massively improve the reproducibility of your work:
* <font size = 20>Write some scripts</font>
* <font size = 20>Share your data and code</font>
* <font size = 20>Use version control</font> (after coffee)
