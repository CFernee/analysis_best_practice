---
title: "Data analysis best practice"
subtitle: "Data managment"
author: "Robert Arbon"
highlighter: highlight.js
hitheme: tomorrow
job: Data scientist, Jean Golding Institute
logo: 
mode: selfcontained
framework: io2012
url:
  assets: ../assets
  lib: ../librariesNew
widgets: mathjax
output: ioslides_presentation
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
library(knitr)
library(tidyverse)
library(FSA)


options(width = 100)
opts_chunk$set(eval=T, results = 'markup', include=T, message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
# knit_hooks$set(inline = function(x) {
#   if(is.numeric(x)) {
#     round(x, getOption('digits'))
#   } else {
#     paste(as.character(x), collapse = ', ')
#   }
# })
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Overview 

1. FAIR principles
1. Storing your data and research
1. Backing up
1. Metadata
3. Tidy data
2. Retrieving data
4. File formats

---

## Your research

* Your research includes: 
  * Raw data
  * Processed data 
  * Statistical analyses and models
  * Reports, presentations, papers
* Your data will be handled slightly different but general principles apply to all of your research. 


---

## FAIR principles
Your research should be FAIR

* **F**indable
  * accurate abstracts and keywords 
  * appears in search engines
  * use well known repositories
* **A**ccessible
  * open access if possible (ethics may prevent this)
  * use web interfaces and APIs
* **I**nteroperable
  * Adhear to open standards, nomenclature and common syntax
  * Good documentation
  * Open source software
* **R**eusable
  * Can someone re-use your data, analysis and understand your reports?
  * Use permissable licences (e.g. MIT)

---

## Storing data

* You've collected your data, it can be in: 
  * Physical format e.g. paperwork, print-outs, samples. 
  * Electronic data e.g. simulation results, electronic surveys, public datasets
* If not already in electronic format you'll transcribe the results to electronic format. 
* Wherever possible, **KEEP YOUR RAW DATA**
  * Physical fomat: keep it in a box, fridge, cupboard under the stairs etc. 
  * Electronic format:
      1. Keep in a **READ ONLY** subdirectory
      2. The directory should be located on a **MANAGED STORAGE DATA** cluster (at least)

If it's just stored on your personal computer **YOU DON'T HAVE IT**


---

## Storage locations - Bristol University

* Research Data Storage Facility, [RDSF](http://www.bristol.ac.uk/acrc/research-data-storage-facility/)
  * good for long-term archiving of data
  * DOI available
  * your PI needs to apply for space
  * you can map the drive to your local machine (ask IT services for help)
  * there is a mechanism for sharing (we'll be using it later)


* [Microsoft OneDrive for Business](https://uob.sharepoint.com/sites/systemsupport/SitePages/onedrive-home.aspx)
  * good for backing up all files (not just data) and sharing with colleagues
  * personal space of 5TB
  * you can sync the drive with your local machine
  * not compatible with OSF (only personal OneDrive accounts are)


---

## Storage locations - Personal 

* [Google Drive](https://www.google.co.uk/drive/)/[Dropbox](https://www.dropbox.com/)
  * Similar to OneDrive but only ~15GB free ( ~ Â£80/year/TB)
  * Compatible with OSF
  
* [Figshare](https://figshare.com/)
  * Similar to RDSF
  * 5GB free
  * DOI available

* OSF has a default storage option
  * Not recommended as sole place of storage
  * Users can add-in other storage services like Figshare, Google Drive etc. 

* Many other services - please see [OSF-FAQ](http://help.osf.io/m/faqs/l/726460-faqs#what-is-the-cap-on-data-per-user-or-per-project)

---

## Backing up

* Backing up your research is **NON NEGOTIABLE**
* Redundancy is key: 
  1. Use rolling back up of whole disk using:
    * external hard drive, e.g. Time Machine for Mac, Backup and Restore for Windows 10
    * cloud storage, e.g. [Backblaze](https://www.backblaze.com/cloud-backup.html)
  2. Use OneDrive to back up all important directories
  3. Keep important research data on RDSF
  4. Back important code on Github

---

## Metadata

* Metadata = data about the data, e.g.
  * units of measurement
  * variable explanation
  * how/when/where it was collected
  * software/hardware used
  * author information (contact details)
* Should be kept in separate files but in same directory as data 
* Often called a *data dictionary*

---

## Tidy data

* Most data sets need **cleaning** before they can be used. 
* **Cleaning** involves:
  * Getting data into tables
  * Getting formats correct e.g., dates and times, units of measurement
  * Grouping observations together in meaningful ways
  * Tidying data: structuring datasets to facilitate analysis
* **Tidy data** is a set of standards for organizig data values within a dataset.
* You can read about it [here]()

---

## Tidy data  - Semantics

* **Values**: entries in a data set
  * Ranged: they have a consistent metric e.g. integers, floats
  * Factors: No consistent metric e.g.
    * Ordinal: ordered e.g. Likert scale
    * Categorical: unordered e.g. male/female/trans
* **Variable**: measures an attribute
* **Observation**: all values measured on a unit (e.g. subject, time) across attributes. 
* Each value belongs to an observation and a variable. 


---

## Tidy data - Semantics quiz

> - Q: Are `height` and `weight` variables or observations?

> - A: They are both variables. 

> - Q: Are `height` and `width` variables or observations?

> - A: They could be both variables **or** observations of a `dimension` variable. 

> - Conclusion: not always easy to make rules for what is a variable and what is an observation. 

---

## Tidy data - the rules

1. Every variable is a column
2. Every observation is a row
3. Every type of observational unit forms a table

* Variable vs observation - if there is ambiguity: 
  * **Wide format**: `area = height * width` then `height` and `width` are variables
  * **Long format**: summarise average height to average width then `height` and `width` are observations. 
  * more on this when plotting. 

--- 

## Tidy data - tools

```{r, echo=F, out.width="100%"}
include_graphics("fig/tidyverse.png")
```

---{.build}

### `tidyr` - example 1

* Scores after 50 sec, 100 sec,..., 350 sec in a game
* Q: Why isn't this tidy?

```{r, echo=F, eval=T}
scores <- read.table(header = TRUE, check.names = FALSE, text = "
   Name    50  100  150  200  250  300  350
   Carla  1.2  1.8  2.2  2.3  3.0  2.5  1.8
   Mace   1.5  1.1  1.9  2.0  3.6  3.0  2.5
   Lea    1.7  1.6  2.3  2.7  2.6  2.2  2.6
   Karen  1.3  1.7  1.9  2.2  3.2  1.5  1.9")
scores
```

> - A: the columns are observations!

---

### `tidyr` - example 1

* use [`gather`](https://tidyr.tidyverse.org/reference/gather.html) to make tidy:

```{r}
scores %>%
  # Gather ALL columns and give default names to columns
  gather() %>%
  headtail()
```

---

### `tidyr` - example 1

* use [`gather`](https://tidyr.tidyverse.org/reference/gather.html) to make tidy:

```{r}
scores %>%
  # Gather all columns and give custom names to columns
  gather(key="MyVariable", value="MyValue") %>%
  headtail()
```

---

### `tidyr` - example 1

* use [`gather`](https://tidyr.tidyverse.org/reference/gather.html) to make tidy:

```{r}
scores %>%
  # Gather all columns except 'Name' and give custom names to columns
  gather(key="Time", value="Score", -Name) %>%
  headtail()
```

---

### `tidyr` - example 2

* Q: What's wrong with this table of counts of males (`m`) and females (`f`) of different ages (`0.15` = 0 - 15) in different states

```{r, echo=F}
df <- data.frame(state=c('MA', 'NY', 'CN', 'OH'),
                 'm0-15' = sample(1:10, 4, replace=T),
                 'm16-60' = sample(1:10, 4, replace=T),
                 'f0-15' = sample(1:10, 4, replace=T),
                 'f16-60' = sample(1:10, 4, replace=T), check.names = F)
df
```

> - A: Columns as variables AND multiple variables in a column

---

### `tidyr` - example 2

* First use [`gather`](https://tidyr.tidyverse.org/reference/gather.html)

```{r}
df %>%
  gather(key='sex-age', value='count', -state) %>%
  headtail()
```

---

### `tidyr` - example 2

* Then use  [`separate`](https://tidyr.tidyverse.org/reference/separate.html)

```{r}
df %>%
  gather(key='sex-age', value='count', -state) %>%
  separate(col='sex-age', into=c('sex', 'age'), sep=1) %>%
  headtail()
```

---

## Retrieving data

* If you've retrieved data - put a note to say where it's from:
```{r, echo=F,out.width="75%"}
include_graphics('fig/source.png')
```

* Even better, write a script that downloads it: 

```r
library(data.table)
file_name <- 'dili_from_anti_TB_treatment.csv'
base_url <- 'http://data.bris.ac.uk/datasets/1vdt21e4mhxxd27hso89cqmhhh/'
file_url <- paste0(base_url, file_name)
df <- fread(file_url)
save(df, file='local/directory/path')
```

---

## File formats

* File formats affect interoperability and accessibility. 
* Proprietory formats for data (e.g. custom binary format) or analysis scripts lower interoperability
* For small, simple data text based formats work well e.g. `.csv`
* For structured data use [`XML`](https://www.w3schools.com/xml/xml_whatis.asp) or  [`json`](https://www.json.org/) or database format e.g. [`mySQL`](https://www.mysql.com/)
* For large data sets use things like [`HDF5`](https://support.hdfgroup.org/HDF5/) which are usable without loading into memory. 
